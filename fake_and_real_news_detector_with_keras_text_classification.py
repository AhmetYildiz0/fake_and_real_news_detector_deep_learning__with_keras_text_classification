# -*- coding: utf-8 -*-
"""fake_and_real_news_detector_with_keras_text_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VO35qdOYfIVfQwEMSoUOp-iyBCIYLd73
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir("/content/drive/MyDrive/yapay_zeka_video/S-003-Fake-and-Real-News-Detector")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import io
import json
import string
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from collections import Counter

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# dataset downloaded : https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset
# upload data

temp_fake=pd.read_csv("fake_real_news_dataset/Fake.csv")
temp_fake['status']=0 #Fake

temp_real=pd.read_csv("fake_real_news_dataset/True.csv")
temp_real['status']=1 #Real

data=pd.concat([temp_fake,temp_real])
data['text']=data['title']+" "+data['text']
data.drop(['title','subject','date'],axis=1,inplace=True)

data = data.sample(frac=1, random_state=42).reset_index(drop=True)

del temp_fake,temp_real
data.head(10)

# analyze and visualize data
print("\n<======Info======>\n")
print(data.info())
print("\n<======Describe======>\n")
print(data.describe())
print("\n<======NA size======>\n")
print(data.isna().sum())

#preprossesing data
def remove_punctuation(text):
  translator = str.maketrans("", "", string.punctuation)
  return text.translate(translator)

# remove_stopwords değiştir

# stop = set(stopwords.words("english"))
# def remove_stopwords(text):
#   filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]
#   return " ".join(filtered_words)

data["text"] = data["text"].map(remove_punctuation)
# data["text"] = data["text"].map(remove_stopwords)

# Count unique words
def counter_word(text_col):
  count = Counter()
  for text in text_col.values:
    for word in text.split():
      count[word] += 1
  return count


counter = counter_word(data["text"])
num_unique_words = len(counter)

# Split dataset into training and validation set
train_size = int(data.shape[0] * 0.8)

train_data = data[:train_size]
val_data = data[train_size:]

# split text and labels
train_sentences = train_data['text'].to_numpy()
train_labels = train_data['status'].to_numpy()
val_sentences = val_data['text'].to_numpy()
val_labels = val_data['status'].to_numpy()

# vectorize a text corpus by turning each text into a sequence of integers
tokenizer = Tokenizer(num_words=num_unique_words)
tokenizer.fit_on_texts(train_sentences) # fit only to training
# each word has unique index
word_index = tokenizer.word_index

train_sequences = tokenizer.texts_to_sequences(train_sentences)
val_sequences = tokenizer.texts_to_sequences(val_sentences)

# Max number of words in a sequence
max_length = 600

train_padded = pad_sequences(train_sequences, maxlen=max_length, padding="post", truncating="post")
val_padded = pad_sequences(val_sequences, maxlen=max_length, padding="post", truncating="post")

# flip (key, value)
reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])

def decode(sequence):
  return " ".join([reverse_word_index.get(idx, "?") for idx in sequence])

# create model
model = tf.keras.models.Sequential()
# model.add(layers.InputLayer(input_shape=(train_padded.shape[-1],)))
model.add(layers.Embedding(num_unique_words, 32, input_length=max_length))

model.add(layers.LSTM(128,dropout=0.2))
# model.add(layers.Dropout(0.0))

model.add(layers.Dense(1, activation="sigmoid"))

# from_logits=False değiştir
loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)
optim = tf.keras.optimizers.Adam(learning_rate=0.001)
metrics = ["accuracy"]

model.compile(loss=loss, optimizer=optim, metrics=metrics)

model.summary()

# train_labels = tf.one_hot(train_labels, depth=1)
# val_labels = tf.one_hot(val_labels, depth=1)

# train the model
model.fit(train_padded, train_labels, epochs=20,batch_size=1024, validation_data=(val_padded, val_labels))

# evaluate model
evaluate_data=model.evaluate(train_padded, train_labels,verbose=0,batch_size=2048)
print("Trian :",evaluate_data)
evaluate_data=model.evaluate(val_padded, val_labels,batch_size=2048,verbose=0)
print("Trian :",evaluate_data)

# save model
model.save("fake_and_real_news_detector.h5")

# save tokinezer
tokenizer_json = tokenizer.to_json()
with io.open('tokenizer.json', 'w', encoding='utf-8') as f:
  f.write(json.dumps(tokenizer_json, ensure_ascii=False))