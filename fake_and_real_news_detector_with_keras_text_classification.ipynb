{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake_and_real_news_detector_with_keras_text_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1flwy1V8R3A"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/yapay_zeka_video/S-003-Fake-and-Real-News-Detector\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import json\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "XjzdbN8V9GlL",
        "outputId": "df81e6b6-9a93-47b9-91cf-69bf08da5be3"
      },
      "source": [
        "# dataset downloaded : https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset\n",
        "# upload data\n",
        "\n",
        "temp_fake=pd.read_csv(\"fake_real_news_dataset/Fake.csv\")\n",
        "temp_fake['status']=0 #Fake\n",
        "\n",
        "temp_real=pd.read_csv(\"fake_real_news_dataset/True.csv\")\n",
        "temp_real['status']=1 #Real\n",
        "\n",
        "data=pd.concat([temp_fake,temp_real])\n",
        "data['text']=data['title']+\" \"+data['text']\n",
        "data.drop(['title','subject','date'],axis=1,inplace=True)\n",
        "\n",
        "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "del temp_fake,temp_real\n",
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ben Stein Calls Out 9th Circuit Court: Committ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Trump drops Steve Bannon from National Securit...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Puerto Rico expects U.S. to lift Jones Act shi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>OOPS: Trump Just Accidentally Confirmed He Le...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Donald Trump heads for Scotland to reopen a go...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Paul Ryan Responds To Dem’s Sit-In On Gun Con...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>AWESOME! DIAMOND AND SILK Rip Into The Press: ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>STAND UP AND CHEER! UKIP Party Leader SLAMS Ge...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>North Korea shows no sign it is serious about ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Trump signals willingness to raise U.S. minimu...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  status\n",
              "0  Ben Stein Calls Out 9th Circuit Court: Committ...       0\n",
              "1  Trump drops Steve Bannon from National Securit...       1\n",
              "2  Puerto Rico expects U.S. to lift Jones Act shi...       1\n",
              "3   OOPS: Trump Just Accidentally Confirmed He Le...       0\n",
              "4  Donald Trump heads for Scotland to reopen a go...       1\n",
              "5   Paul Ryan Responds To Dem’s Sit-In On Gun Con...       0\n",
              "6  AWESOME! DIAMOND AND SILK Rip Into The Press: ...       0\n",
              "7  STAND UP AND CHEER! UKIP Party Leader SLAMS Ge...       0\n",
              "8  North Korea shows no sign it is serious about ...       1\n",
              "9  Trump signals willingness to raise U.S. minimu...       1"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo5l4OGnA6Uv",
        "outputId": "8a70c554-bf38-43cb-99cc-eeba116bb757"
      },
      "source": [
        "# analyze and visualize data\n",
        "print(\"\\n<======Info======>\\n\")\n",
        "print(data.info())\n",
        "print(\"\\n<======Describe======>\\n\")\n",
        "print(data.describe())\n",
        "print(\"\\n<======NA size======>\\n\")\n",
        "print(data.isna().sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<======Info======>\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 44898 entries, 0 to 44897\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    44898 non-null  object\n",
            " 1   status  44898 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 701.7+ KB\n",
            "None\n",
            "\n",
            "<======Describe======>\n",
            "\n",
            "             status\n",
            "count  44898.000000\n",
            "mean       0.477015\n",
            "std        0.499477\n",
            "min        0.000000\n",
            "25%        0.000000\n",
            "50%        0.000000\n",
            "75%        1.000000\n",
            "max        1.000000\n",
            "\n",
            "<======NA size======>\n",
            "\n",
            "text      0\n",
            "status    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuenuW4UBFHF"
      },
      "source": [
        "#preprossesing data\n",
        "def remove_punctuation(text):\n",
        "  translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "  return text.translate(translator)\n",
        "\n",
        "# remove_stopwords değiştir\n",
        "\n",
        "# stop = set(stopwords.words(\"english\"))\n",
        "# def remove_stopwords(text):\n",
        "#   filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
        "#   return \" \".join(filtered_words)\n",
        "\n",
        "data[\"text\"] = data[\"text\"].map(remove_punctuation)\n",
        "# data[\"text\"] = data[\"text\"].map(remove_stopwords)\n",
        "\n",
        "# Count unique words\n",
        "def counter_word(text_col):\n",
        "  count = Counter()\n",
        "  for text in text_col.values:\n",
        "    for word in text.split():\n",
        "      count[word] += 1\n",
        "  return count\n",
        "\n",
        "\n",
        "counter = counter_word(data[\"text\"])\n",
        "num_unique_words = len(counter)\n",
        "\n",
        "# Split dataset into training and validation set\n",
        "train_size = int(data.shape[0] * 0.8)\n",
        "\n",
        "train_data = data[:train_size]\n",
        "val_data = data[train_size:]\n",
        "\n",
        "# split text and labels\n",
        "train_sentences = train_data['text'].to_numpy()\n",
        "train_labels = train_data['status'].to_numpy()\n",
        "val_sentences = val_data['text'].to_numpy()\n",
        "val_labels = val_data['status'].to_numpy()\n",
        "\n",
        "# vectorize a text corpus by turning each text into a sequence of integers\n",
        "tokenizer = Tokenizer(num_words=num_unique_words)\n",
        "tokenizer.fit_on_texts(train_sentences) # fit only to training\n",
        "# each word has unique index\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
        "\n",
        "# Max number of words in a sequence\n",
        "max_length = 600\n",
        "\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "# flip (key, value)\n",
        "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])\n",
        "\n",
        "def decode(sequence):\n",
        "  return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_m9G8DaBUia",
        "outputId": "e70415a7-084e-4d70-8632-9cbd2a51f889"
      },
      "source": [
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "# model.add(layers.InputLayer(input_shape=(train_padded.shape[-1],)))\n",
        "model.add(layers.Embedding(num_unique_words, 32, input_length=max_length))\n",
        "\n",
        "model.add(layers.LSTM(128,dropout=0.2))\n",
        "# model.add(layers.Dropout(0.0))\n",
        "\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "# from_logits=False değiştir\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "metrics = [\"accuracy\"]\n",
        "\n",
        "model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_12 (Embedding)    (None, 600, 32)           9292480   \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (None, 128)               82432     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,375,041\n",
            "Trainable params: 9,375,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRUtQryMfYV0"
      },
      "source": [
        "# train_labels = tf.one_hot(train_labels, depth=1)\n",
        "# val_labels = tf.one_hot(val_labels, depth=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG8NYOOiYjr5",
        "outputId": "0a50a3e9-5ad2-4ed6-c365-38af0dde1e39"
      },
      "source": [
        "# train the model\n",
        "model.fit(train_padded, train_labels, epochs=20,batch_size=1024, validation_data=(val_padded, val_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "36/36 [==============================] - 20s 489ms/step - loss: 0.6909 - accuracy: 0.5300 - val_loss: 0.6873 - val_accuracy: 0.5460\n",
            "Epoch 2/20\n",
            "36/36 [==============================] - 17s 466ms/step - loss: 0.6971 - accuracy: 0.5592 - val_loss: 0.6881 - val_accuracy: 0.5545\n",
            "Epoch 3/20\n",
            "36/36 [==============================] - 17s 469ms/step - loss: 0.6869 - accuracy: 0.5651 - val_loss: 0.6866 - val_accuracy: 0.5655\n",
            "Epoch 4/20\n",
            "36/36 [==============================] - 17s 464ms/step - loss: 0.6829 - accuracy: 0.5880 - val_loss: 0.6830 - val_accuracy: 0.5766\n",
            "Epoch 5/20\n",
            "36/36 [==============================] - 17s 470ms/step - loss: 0.5876 - accuracy: 0.6955 - val_loss: 0.4434 - val_accuracy: 0.8514\n",
            "Epoch 6/20\n",
            "36/36 [==============================] - 17s 473ms/step - loss: 0.4297 - accuracy: 0.8420 - val_loss: 0.4274 - val_accuracy: 0.8398\n",
            "Epoch 7/20\n",
            "36/36 [==============================] - 17s 474ms/step - loss: 0.3982 - accuracy: 0.8542 - val_loss: 0.3878 - val_accuracy: 0.8650\n",
            "Epoch 8/20\n",
            "36/36 [==============================] - 17s 474ms/step - loss: 0.3488 - accuracy: 0.8823 - val_loss: 0.3280 - val_accuracy: 0.8953\n",
            "Epoch 9/20\n",
            "36/36 [==============================] - 17s 466ms/step - loss: 0.3016 - accuracy: 0.9062 - val_loss: 0.3094 - val_accuracy: 0.9068\n",
            "Epoch 10/20\n",
            "36/36 [==============================] - 17s 465ms/step - loss: 0.3216 - accuracy: 0.8755 - val_loss: 0.6645 - val_accuracy: 0.5791\n",
            "Epoch 11/20\n",
            "36/36 [==============================] - 17s 470ms/step - loss: 0.6348 - accuracy: 0.5903 - val_loss: 0.6434 - val_accuracy: 0.6248\n",
            "Epoch 12/20\n",
            "36/36 [==============================] - 17s 464ms/step - loss: 0.6315 - accuracy: 0.6307 - val_loss: 0.6608 - val_accuracy: 0.5758\n",
            "Epoch 13/20\n",
            "36/36 [==============================] - 17s 469ms/step - loss: 0.6170 - accuracy: 0.6081 - val_loss: 0.6384 - val_accuracy: 0.5938\n",
            "Epoch 14/20\n",
            "36/36 [==============================] - 17s 467ms/step - loss: 0.4705 - accuracy: 0.7740 - val_loss: 0.2583 - val_accuracy: 0.9281\n",
            "Epoch 15/20\n",
            "36/36 [==============================] - 17s 466ms/step - loss: 0.2169 - accuracy: 0.9406 - val_loss: 0.2092 - val_accuracy: 0.9443\n",
            "Epoch 16/20\n",
            "36/36 [==============================] - 17s 463ms/step - loss: 0.1751 - accuracy: 0.9543 - val_loss: 0.2030 - val_accuracy: 0.9460\n",
            "Epoch 17/20\n",
            "36/36 [==============================] - 17s 474ms/step - loss: 0.1749 - accuracy: 0.9547 - val_loss: 0.1928 - val_accuracy: 0.9504\n",
            "Epoch 18/20\n",
            "36/36 [==============================] - 17s 473ms/step - loss: 0.1643 - accuracy: 0.9587 - val_loss: 0.2361 - val_accuracy: 0.9389\n",
            "Epoch 19/20\n",
            "36/36 [==============================] - 17s 471ms/step - loss: 0.1781 - accuracy: 0.9555 - val_loss: 0.1852 - val_accuracy: 0.9524\n",
            "Epoch 20/20\n",
            "36/36 [==============================] - 17s 472ms/step - loss: 0.1525 - accuracy: 0.9630 - val_loss: 0.1856 - val_accuracy: 0.9521\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f60c2d48bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CESdoI53BWWD",
        "outputId": "ca794c3a-2e86-42a4-f656-3ebeef83a8d5"
      },
      "source": [
        "# evaluate model\n",
        "evaluate_data=model.evaluate(train_padded, train_labels,verbose=0,batch_size=2048)\n",
        "print(\"Trian :\",evaluate_data)\n",
        "evaluate_data=model.evaluate(val_padded, val_labels,batch_size=2048,verbose=0)\n",
        "print(\"Trian :\",evaluate_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trian : [0.15426617860794067, 0.9628041386604309]\n",
            "Trian : [0.185619056224823, 0.9521158337593079]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDDQZ9IRBY7G"
      },
      "source": [
        "# save model\n",
        "model.save(\"fake_and_real_news_detector.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miEuJMpDoxcq"
      },
      "source": [
        "# save tokinezer\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with io.open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
        "  f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}